from sklearn.metrics.pairwise import cosine_similarity
from scipy.stats import pearsonr
import numpy as np
import pandas as pd
from DCU_performance_analysis import Performance_data_analysis

class Data_similarity_analysis:
    def read_and_nomalize_data(self, dir, p):
        data = p.read_performance_data(self, dir)

        # 对数据分组，并且求均值
        data_groupby_kernel_mean = p.groupby_and_mean_data(data)
        data_groupby_kernel_mean = p.data_groupby_kernel_mean.drop(labels = ['Index', 'KernelName'], axis=1)
        data_groupby_kernel_mean_norm = p.normalize_data(data_groupby_kernel_mean)

        return data_groupby_kernel_mean_norm

    def analyse_different_dcu_in_one_node(self, node_id, dcu_data_list, dcu_number_per_node, similarity_threshold):
        similarity_array = np.ones([dcu_number_per_node, dcu_number_per_node])
        for i in range(0, dcu_number_per_node):
            for j in range(i, dcu_number_per_node):
                # 行数或列数不相同，相似度直接设置成0
                if dcu_data_list[i].shape[0] != dcu_data_list[j].shape[0] or dcu_data_list[i].shape[1] != dcu_data_list[j].shape[1]:
                    similarity_array[i][j] = 0.0
                    continue

                similarity_array[i][j] = 1.0
                # 取所有皮尔逊系数中的最小值作为当前两个DCU之间的相似度
                for row in range(dcu_data_list[i].shape[0]):
                    similarity_array[i][j] = min(pearsonr(dcu_data_list[i].iloc[row], dcu_data_list[j].iloc[row])[0], similarity_array[i][j])
                similarity_array[i][j] = round(similarity_array[i][j], 4)
                if similarity_array[i][j] < similarity_threshold:
                    print("在节点 " + str(node_id) + " 上DCU " + str(i) + " 和DCU " + str(j)  \
                                    + " 之间的硬件计数器相似度为: " + str(similarity_array[i][j]) \ 
                                    + ", 小于相似度阈值(" + str(similarity_threshold) + ").")

    def analyse_different_dcu_in_different_node(self, node_id, pre_dcu_data_list, dcu_data_list, dcu_number_per_node, similarity_threshold):
        # 如果是第一个节点，则无法进行节点间的相似度分析，直接返回
        if node_id == 0:
            return
        
        for dcu_id in range(dcu_number_per_node):
            if pre_dcu_data_list[dcu_id].shape[0] != dcu_data_list[dcu_id].shape[0]:
                print("节点 " + str(node_id - 1) + " 和节点 " + str(node_id) + " 上的DCU " + str(dcu_id) + " 调用核函数个数不同！")
                continue

            min_similarity = 1.0
            row_num = dcu_data_list[dcu_id].shape[0]
            for row in range(row_num):
                min_similarity = min(pearsonr(pre_dcu_data_list[dcu_id].iloc[row], dcu_data_list[dcu_id].iloc[row])[0], min_similarity)
            min_similarity = round(min_similarity, 4)
            if min_similarity < similarity_threshold:
                print("节点 " + str(node_id - 1) + " 和节点 " + str(node_id) + " 上的DCU " + str(dcu_id) \
                                + " 的硬件计数器相似度为: " + str(min_similarity) \ 
                                + ", 小于相似度阈值(" + str(similarity_threshold) + ").")

    def analyse_similarity(self, dir, node_number, dcu_number_per_node, similarity_threshold):
        p = Performance_data_analysis()
        # 遍历所有节点
        file_no = 0
        pre_dcu_data_list = []
        for node_id in range(node_number):
            if node_id != 0:
                pre_data_list = dcu_data_list
            dcu_data_list = []
            # 读取同一个节点上不同DCU的硬件计数器结果（遍历同一个节点上的不同DCU）
            for dcu_id in range(dcu_number_per_node):
                data = read_and_nomalize_data(dir + '/counter' + str(file_no) + '.csv', p)
                # 去除nan的列
                data = data.dropna(axis=1,how='any')
                dcu_data_list.append(data)

            file_no += dcu_number_per_node

            analyse_different_dcu_in_one_node(node_id, dcu_data_list, dcu_number_per_node, similarity_threshold)
            analyse_different_dcu_in_different_node(node_id, pre_dcu_data_list, dcu_data_list, dcu_number_per_node, similarity_threshold)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='hardware configuration information')
    parser.add_argument("-N", "--node_num", dest='node_num', type=int, help='The total number of compute node.')
    parser.add_argument("-n", "--dcu_num_per_node", dest='dcu_num_per_node', type=int, help='The number of DCU per compute node.')
    parser.add_argument("-d", "--dir_include_data", dest='dir_include_data', type=str, help='The filepath of folder/directory includes performance data.')
    parser.add_argument("-t", "--similarity_threshold", dest='similarity_threshold', type=float, \
                        help='The similarity threshold, similarity value between two DCUs smaller than it represents the performance result generated by these DCUs are not similar.')

    args = parser.parse_args()

    d = Data_similarity_analysis()
    d.analyse_similarity(dir_include_data, node_num, dcu_num_per_node, similarity_threshold)
